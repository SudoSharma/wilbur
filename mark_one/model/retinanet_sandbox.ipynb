{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sandbox for Implementing RetinaNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- start wih pretrined resnet50, maybe from the original paper, or mybe from fastai, doesn't really matter\n",
    "- then add feature pyramid network and finetune for coco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Approach\n",
    "\n",
    "1. Download COCO\n",
    "- Build and train U-Net\n",
    "- Build Feature Pyramid Network (FPN) with ResNet backbone\n",
    "- Build and train RetinaNet with FPN and object detection subnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.datasets import *\n",
    "from fastai.callbacks import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/sudosharma/projects/data/coco_sample/annotations'),\n",
       " PosixPath('/home/sudosharma/projects/data/coco_sample/train_sample')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.COCO_SAMPLE)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You can deactivate this warning by passing `no_check=True`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sudosharma/miniconda3/envs/wilbur/lib/python3.7/site-packages/fastai/basic_data.py:257: UserWarning: It's not possible to collate samples of your dataset together in a batch.\n",
      "Shapes of the inputs/targets:\n",
      "[[torch.Size([3, 640, 480]), torch.Size([3, 441, 640]), torch.Size([3, 427, 640]), torch.Size([3, 640, 480]), torch.Size([3, 480, 640]), torch.Size([3, 375, 500]), torch.Size([3, 480, 640]), torch.Size([3, 640, 361]), torch.Size([3, 338, 640]), torch.Size([3, 427, 640]), torch.Size([3, 427, 640]), torch.Size([3, 478, 640]), torch.Size([3, 612, 612]), torch.Size([3, 500, 332]), torch.Size([3, 427, 640]), torch.Size([3, 480, 640]), torch.Size([3, 421, 640]), torch.Size([3, 640, 456]), torch.Size([3, 375, 500]), torch.Size([3, 480, 640]), torch.Size([3, 425, 640]), torch.Size([3, 428, 640]), torch.Size([3, 427, 640]), torch.Size([3, 640, 480]), torch.Size([3, 640, 427]), torch.Size([3, 640, 427]), torch.Size([3, 426, 640]), torch.Size([3, 640, 427]), torch.Size([3, 563, 640]), torch.Size([3, 478, 640]), torch.Size([3, 480, 640]), torch.Size([3, 640, 427]), torch.Size([3, 480, 640]), torch.Size([3, 480, 640]), torch.Size([3, 427, 640]), torch.Size([3, 480, 640]), torch.Size([3, 563, 640]), torch.Size([3, 480, 640]), torch.Size([3, 425, 640]), torch.Size([3, 603, 640]), torch.Size([3, 423, 640]), torch.Size([3, 333, 500]), torch.Size([3, 640, 427]), torch.Size([3, 335, 500]), torch.Size([3, 428, 640]), torch.Size([3, 427, 640]), torch.Size([3, 640, 426]), torch.Size([3, 427, 640]), torch.Size([3, 640, 426]), torch.Size([3, 478, 640]), torch.Size([3, 500, 346]), torch.Size([3, 427, 640]), torch.Size([3, 480, 640]), torch.Size([3, 480, 640]), torch.Size([3, 640, 424]), torch.Size([3, 480, 640]), torch.Size([3, 480, 640]), torch.Size([3, 480, 640]), torch.Size([3, 640, 480]), torch.Size([3, 355, 500]), torch.Size([3, 480, 640]), torch.Size([3, 640, 480]), torch.Size([3, 330, 500]), torch.Size([3, 426, 640])], [(), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), (), ()]]\n",
      "  warn(message)\n"
     ]
    }
   ],
   "source": [
    "tfms = get_transforms()\n",
    "data = ImageDataBunch.from_folder(path, train='train_sample', ds_tfms=tfms, valid_pct=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv(ic, oc, k=3, s=1, actn=True):\n",
    "    layers = [nn.Conv2d(ic, oc, k, stride=s, padding=0)]\n",
    "    if actn: layers.append(nn.ReLU(True))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "def conv_block(ic, oc): return nn.Sequential(conv(ic, oc), conv(oc, oc), nn.MaxPool2d(2, stride=2))\n",
    "def contraction(cs): return nn.Sequential(*[conv_block(cs[i], cs[i+1]) for i in range(len(cs)-1)])\n",
    "\n",
    "class SaveMap():\n",
    "    def __init__(self, m):\n",
    "        self.hook = m.register_forward_hook(self.hook_fn)\n",
    "        \n",
    "    def hook_fn(self, module, input, output): self.map.append(out)\n",
    "    def close(self): self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (1): ReLU(inplace)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = [3, 64, 128, 256, 512, 1024]\n",
    "model = contraction(cs)\n",
    "model[0][1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
